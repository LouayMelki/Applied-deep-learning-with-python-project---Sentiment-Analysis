{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1052467e-c73d-41ee-af01-7b95e0e1fe6d",
   "metadata": {},
   "source": [
    "# Baseline model for IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d5cd3b-03a5-46b9-a70b-7f2f4849cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ngtna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75c6788-15ee-4358-a40b-c1b27b6202b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb106c-0114-4029-9a40-4073943f7d81",
   "metadata": {},
   "source": [
    "## Load the dataframe and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593d84ab-e0aa-455f-8a48-b49b1daa29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"IMDB Dataset.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3cea89-e8f5-4560-9614-615a4375fc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>After reading the original play I thought it w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "6801  After reading the original play I thought it w...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c81e4fd-2a76-46b2-8489-9291627d3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438c4fb9-65a7-4d41-99d4-36c1e0fea953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1620ab7-362d-4035-b274-2f5624760c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts(normalize=True).plot.bar()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title(\"Ratio of negative and positive samples in the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52db18c-d11b-49fc-90c4-b658ac713ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"sentiment\"].apply(lambda x: 0 if x==\"negative\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f246eb7-b95d-42a5-86d2-17319b14e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1afdf3c-a5e8-4267-801e-a0f59283e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df[\"review\"].to_list()\n",
    "target_list = df[\"target\"].to_list()\n",
    "text_train, text_test, target_train, target_test = train_test_split(\n",
    "   text_list, target_list, test_size=0.2, stratify=target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b36ea-c9ba-4e82-addc-618754855eb8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b0dfa83-63e4-4c3b-8759-a6476ef5f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data, categories, vocab = None, max_length = 100, min_freq = 5):\n",
    "        self.data = data      \n",
    "        # Set the maximum length we will keep for the sequences\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Allow to import a vocabulary (for valid/test datasets, that will use the training vocabulary)\n",
    "        if vocab is not None:\n",
    "            self.word2idx, self.idx2word = vocab\n",
    "        else:\n",
    "            # If no vocabulary imported, build it (and reverse)\n",
    "            self.word2idx, self.idx2word = self.build_vocab(self.data, min_freq)\n",
    "        \n",
    "        # We then need to tokenize the data .. \n",
    "        tokenized_data = [word_tokenize(elem) for elem in self.data] # To complete\n",
    "        # Transform words into lists of indexes ... (use the .get() method to redirect unknown words to the UNK token)\n",
    "        indexed_data = []\n",
    "        for token in tokenized_data:\n",
    "            idx=[]\n",
    "            for elem in token:\n",
    "                if self.word2idx.get(elem)==None:\n",
    "                    idx.append(self.word2idx.get('UNK'))\n",
    "                else:\n",
    "                    idx.append(self.word2idx.get(elem))\n",
    "            indexed_data.append(idx)\n",
    "         # To complete\n",
    "        # And transform this list of lists into a list of Pytorch LongTensors\n",
    "        tensor_data =[torch.LongTensor(elem) for elem in indexed_data] \n",
    "        tensor_y = torch.FloatTensor(categories)   # To complete\n",
    "        cut_tensor_data =[elem[:max_length] for elem in tensor_data] # To complete\n",
    "        \n",
    "        # Now, we need to use the pad_sequence function to have the whole dataset represented as one tensor,\n",
    "        # containing sequences of the same length. We choose the padding_value to be 0, the we want the\n",
    "        # batch dimension to be the first dimension \n",
    "        self.tensor_data = pad_sequence(cut_tensor_data, batch_first=True, padding_value=0)\n",
    "        self.tensor_y = tensor_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.tensor_data[idx], self.tensor_y[idx] \n",
    "    \n",
    "    def build_vocab(self, corpus, count_threshold):\n",
    "        \"\"\"\n",
    "        Same as in the previous TP: we want to output word_index, a dictionary containing words \n",
    "        and their corresponding indexes as {word : indexes} \n",
    "        But we also want the reverse, which is a dictionary {indexes: word}\n",
    "        Don't forget to add a UNK token that we need when encountering unknown words\n",
    "        We also choose '0' to represent the padding index, so begin the vocabulary index at 1 ! \n",
    "        \"\"\"\n",
    "        word_counts = {}\n",
    "        # create list of all elements in corpus in corpus\n",
    "        corp=[]\n",
    "        for element in corpus:\n",
    "            corp.append(word_tokenize(element))\n",
    "        for text in corp:\n",
    "            for word in text:\n",
    "                word_counts[word.lower()]=word_counts.get(word.lower(),0)+text.count(word)\n",
    "        keys=sorted(word_counts) #sort the words in alphabetical order \n",
    "        filtered_word_counts = {}\n",
    "        for key in keys:\n",
    "            if (word_counts[key]>=count_threshold): # retrieve words with occurences higher than count_threshold\n",
    "                filtered_word_counts[key]=word_counts[key]\n",
    "        word_index={}\n",
    "        i=1\n",
    "        for key in filtered_word_counts.keys():\n",
    "            word_index[key]=i\n",
    "            i+=1 \n",
    "        word_index['UNK']=i  #adding UNK \n",
    "        idx_word={idx:word for word,idx in word_index.items()}\n",
    "        return word_index, idx_word\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        # A simple way to get the training vocab when building the valid/test \n",
    "        return self.word2idx, self.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db7fd5b-337a-4b2e-979e-3accd6a8b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TextClassificationDataset(text_train, target_train)\n",
    "training_word2idx, training_idx2word = training_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc4d11cd-6ac8-4584-af1e-34ebc46142ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TextClassificationDataset(text_test, target_test, (training_word2idx, training_idx2word))\n",
    "training_dataloader = DataLoader(training_dataset, batch_size = 32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087b9ff-9b82-4cd5-a4d6-a6e31fdf55d5",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a986f4bf-6bd5-4624-befe-6621c54f05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocabulary_size, hidden_dim, embeddings=None, fine_tuning=False):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocabulary_size+1,embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        _,(hidden,cell) = self.lstm(x)\n",
    "        o = self.linear(hidden)\n",
    "        o_logit = torch.squeeze(o, 1)\n",
    "        return(o_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0b23a9-fd73-4d87-9c99-1d6c2ccf26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(300, len(training_word2idx), 32).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4f028-26c9-42ef-ab80-82da9b063c5d",
   "metadata": {},
   "source": [
    "## Train, val, experiment functions (helper functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a142eaf4-b438-40f3-adf4-513759d647c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Training model for one epoch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model will be trained\n",
    "    opt : optimizer \n",
    "    criterion : for loss function\n",
    "    dataloader : input of the model in the form of dataloader\n",
    "    Returns\n",
    "    -------\n",
    "    list of all batch losses in this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model.forward(x)\n",
    "        loss = criterion(pred,y)\n",
    "        loss.backward() \n",
    "        opt.step() \n",
    "        losses.append(loss.item())\n",
    "        # Count the number of correct predictions in the batch - here, you'll need to use the sigmoid\n",
    "        act = torch.sigmoid(pred)\n",
    "        thresh = torch.tensor([0.5])\n",
    "        postact = (act>thresh).float() \n",
    "        num_corrects = sum(y==postact)\n",
    "        acc = 100.0 * num_corrects/len(y)   \n",
    "        if (i%20 == 0):\n",
    "            print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88471048-f8de-4264-96de-680033bc1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion, evalloader):\n",
    "    model.eval()\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(evalloader):\n",
    "            x.to(device)\n",
    "            y.to(device)\n",
    "            pred = model.forward(x) \n",
    "            loss = criterion(pred,y)\n",
    "            # get probability of the prediction \n",
    "            act = torch.sigmoid(pred)\n",
    "            act.to(\"cpu\")                             \n",
    "            thresh = torch.tensor([0.5])\n",
    "            postact = (act>thresh).float() \n",
    "            num_corrects = sum(y==postact) \n",
    "            acc = 100.0 * num_corrects/len(y)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/(i+1), total_epoch_acc/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce2d8933-3619-45d8-9c01-e3b18970efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, opt, criterion, num_epochs = 5, early_stopping = True):\n",
    "    train_losses = []\n",
    "    if early_stopping: \n",
    "        best_valid_loss = 10. \n",
    "    print(\"Beginning training...\")\n",
    "    for e in range(num_epochs):\n",
    "        print(\"Epoch \" + str(e+1) + \":\")\n",
    "        train_losses += train_epoch(model, opt, criterion, training_dataloader)  # Add all elements of a list to the previous list. extend\n",
    "        valid_loss, valid_acc = eval_model(model, criterion, valid_dataloader)\n",
    "        print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n",
    "        if early_stopping:\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            else:\n",
    "                print(\"Early stopping.\")\n",
    "                break  \n",
    "    test_loss, test_acc = eval_model(model, criterion, test_dataloader)\n",
    "    print(\"Epoch \" + str(e+1) + \" : Test loss = \" + str(test_loss) + \"; Test acc = \" + str(test_acc))\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6353bc5-7dbb-45bb-b68b-cd9a4c93c291",
   "metadata": {},
   "source": [
    "## Run and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a2957b2-efc9-465c-96ba-69c5190e9b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35756/3394792900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35756/1201988651.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(model, opt, criterion, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_losses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_dataloader\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Add all elements of a list to the previous list. extend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" : Validation loss = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"; Validation acc = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35756/630816222.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, opt, criterion, dataloader)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35756/3527395552.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\nlpparis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\nlpparis\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\miniforge3\\envs\\nlpparis\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "train_losses = experiment(model, opt, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ecdab-4cd9-4225-be36-3c874d79cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32701285-4bbc-4f69-ab96-929c65ec6463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlpparis] *",
   "language": "python",
   "name": "conda-env-nlpparis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
